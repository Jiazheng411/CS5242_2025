{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02: Vanilla RNN yourself - exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5242_2025_codes/labs_lecture06/lab02_vrnn_yourself'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"file_name\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With or without GPU\n",
    "\n",
    "It is recommended to run this code on GPU:<br> \n",
    "* Time for 1 epoch on CPU : 153 sec ( 2.55 min)<br> \n",
    "* Time for 1 epoch on GPU : 8.4 sec w/ GeForce GTX 1080 Ti <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\")\n",
    "device= torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available with GPU:',torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Penn Tree Bank\n",
    "\n",
    "The tensor train_data consists of 20 columns of 46,479 words.<br>\n",
    "The tensor test_data consists of 20 columns of 4,121 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46479, 20])\n",
      "torch.Size([4121, 20])\n"
     ]
    }
   ],
   "source": [
    "from utils import check_ptb_dataset_exists\n",
    "data_path=check_ptb_dataset_exists()\n",
    "\n",
    "train_data  =  torch.load(data_path+'ptb/train_data.pt')\n",
    "test_data   =  torch.load(data_path+'ptb/test_data.pt')\n",
    "\n",
    "print(  train_data.size()  )\n",
    "print(  test_data.size()   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some constants associated with the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 20\n",
    "\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a recurrent net class\n",
    "\n",
    "Implement the vanilla RNN network without PyTorch:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh ( Rh_{t-1} + V g_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_VRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.R = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        # COMPLETE HERE \n",
    "    \n",
    "    \n",
    "    def forward(self, g_seq, h_init):\n",
    "        # g_seq: (T, B, H)\n",
    "        # h_init: (1, B, H)\n",
    "        t = len(g_seq)           # t: Sequence length\n",
    "        h_seq_list = []          # Will hold T tensors of shape [1, B, H]\n",
    "        h_prev = h_init          # h_prev: [1, B, H]\n",
    "        for i in range(t):\n",
    "            h_current = torch.tanh(self.R(h_prev) + self.V(g_seq[i]))   # [1, B, H] + [B, H] -> [1, B, H] pytorch broadcasting see next cell\n",
    "            h_seq_list.append(h_current)\n",
    "            h_prev = h_current  # [1, B, H]\n",
    "        h_seq = torch.cat(h_seq_list) # [T, B, H]  not if it is torch.stack, then shape is [T, 1, B, H]\n",
    "        h_final = h_current     # [1, B, H]\n",
    "        return h_seq, h_final\n",
    "\n",
    "#answer\n",
    "class my_VRNN_answer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.R = nn.Linear( hidden_size , hidden_size )\n",
    "        self.V = nn.Linear( hidden_size , hidden_size )\n",
    "        \n",
    "    # TA use cat because it already has an extra dimension\n",
    "    def forward(self, g_seq , h_init ):\n",
    "        # g_seq:  (T, B, H)\n",
    "        # h_init: (1, B, H)\n",
    "        V_g_all_t = self.V(g_seq) # (T, B, H)\n",
    "        h_t_pre = h_init    # (1, B, H)\n",
    "        h_seq = []\n",
    "        for V_g_t in V_g_all_t:\n",
    "            R_h_t_pre = self.R(h_t_pre)  # (1, B, H) -> (1, B, H)\n",
    "            h_t = torch.tanh( R_h_t_pre + V_g_t ) # (1, B, H) + (B, H) -> (1, B, H) due to broadcasting\n",
    "            h_seq.append(h_t)\n",
    "            h_t_pre = h_t\n",
    "        h_seq = torch.cat(h_seq) \n",
    "        # h_seq[-1,:,:] is the same as h_seq[-1] and is (B, H) and we need to add an extra dimension to make it (1, B, H)\n",
    "        h_final = h_seq[-1,:,:].unsqueeze(0)\n",
    "        return h_seq , h_final\n",
    "    \n",
    "class three_layer_recurrent_net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(three_layer_recurrent_net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
    "        self.layer2 = my_VRNN( hidden_size )\n",
    "        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n",
    "\n",
    "        \n",
    "    def forward(self, word_seq, h_init ):\n",
    "        \n",
    "        g_seq               =   self.layer1( word_seq )  \n",
    "        h_seq , h_final     =   self.layer2( g_seq , h_init )\n",
    "        score_seq           =   self.layer3( h_seq )\n",
    "        \n",
    "        return score_seq,  h_final \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3]) torch.Size([2, 3]) torch.Size([1, 2, 3]) torch.Size([1, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True],\n",
       "         [True, True, True]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TA's code also works because of pytorch's broadcasting\n",
    "a = torch.rand(1,2,3)\n",
    "b = torch.rand(2,3)\n",
    "c = a+b\n",
    "d = a + b.unsqueeze(0)\n",
    "print(a.size(), b.size(), c.size(), d.size())\n",
    "c == d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the net. Choose the hidden size to be 150. How many parameters in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three_layer_recurrent_net(\n",
      "  (layer1): Embedding(10000, 150)\n",
      "  (layer2): my_VRNN(\n",
      "    (R): Linear(in_features=150, out_features=150, bias=True)\n",
      "    (V): Linear(in_features=150, out_features=150, bias=True)\n",
      "  )\n",
      "  (layer3): Linear(in_features=150, out_features=10000, bias=True)\n",
      ")\n",
      "There are 3055300 (3.06 million) parameters in this neural network\n"
     ]
    }
   ],
   "source": [
    "hidden_size=150\n",
    "\n",
    "net = three_layer_recurrent_net( hidden_size )\n",
    "\n",
    "print(net)\n",
    "\n",
    "utils.display_num_param(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the weights of the networks to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up manually the weights of the embedding module and Linear module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net.layer1.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "net.layer3.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the criterion, as well as the following important hyperparameters: \n",
    "* initial learning rate = 1\n",
    "* sequence length = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "my_lr = 1\n",
    "\n",
    "seq_length = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate the network on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test_set():\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    \n",
    "    h=h.to(device)\n",
    "\n",
    "       \n",
    "    for count in range( 0 , 4120-seq_length ,  seq_length) :\n",
    "               \n",
    "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "        \n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "                                  \n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length ) \n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)\n",
    "        \n",
    "        loss = criterion(  scores ,  minibatch_label )    \n",
    "        \n",
    "        h=h.detach()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    total_loss = running_loss/num_batches \n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do 10 passes through the training set (100 passes would reach 135 on test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 276.6997301578522 \t lr= 1 \t exp(loss)= 523.0824489593288\n",
      "test: exp(loss) =  333.72204578861573\n",
      "\n",
      "epoch= 1 \t time= 522.1988043785095 \t lr= 1 \t exp(loss)= 269.7785354066164\n",
      "test: exp(loss) =  251.1308952285022\n",
      "\n",
      "epoch= 2 \t time= 789.6021013259888 \t lr= 1 \t exp(loss)= 209.4903977679223\n",
      "test: exp(loss) =  211.92724755893207\n",
      "\n",
      "epoch= 3 \t time= 1052.75337266922 \t lr= 1 \t exp(loss)= 178.81547397881758\n",
      "test: exp(loss) =  194.9606635370571\n",
      "\n",
      "epoch= 4 \t time= 1212.4742691516876 \t lr= 0.9090909090909091 \t exp(loss)= 157.38269526974233\n",
      "test: exp(loss) =  182.89685679366127\n",
      "\n",
      "epoch= 5 \t time= 1336.0235283374786 \t lr= 0.8264462809917354 \t exp(loss)= 142.5764237995476\n",
      "test: exp(loss) =  172.2976469233339\n",
      "\n",
      "epoch= 6 \t time= 1457.3349962234497 \t lr= 0.7513148009015777 \t exp(loss)= 131.9495524784308\n",
      "test: exp(loss) =  166.1694411948323\n",
      "\n",
      "epoch= 7 \t time= 1642.4559769630432 \t lr= 0.6830134553650705 \t exp(loss)= 123.662302369137\n",
      "test: exp(loss) =  161.84292280791465\n",
      "\n",
      "epoch= 8 \t time= 1848.835806131363 \t lr= 0.6209213230591549 \t exp(loss)= 117.11768734780112\n",
      "test: exp(loss) =  157.782599874557\n",
      "\n",
      "epoch= 9 \t time= 2065.0600843429565 \t lr= 0.5644739300537771 \t exp(loss)= 111.62208280655945\n",
      "test: exp(loss) =  154.7616202993367\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "    if epoch >= 4:\n",
    "        my_lr = my_lr / 1.1\n",
    "    \n",
    "    # create a new optimizer and give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quantities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    # set the initial h to be the zero vector\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "\n",
    "    # send it to the gpu    \n",
    "    h=h.to(device)\n",
    "    \n",
    "    for count in range( 0 , 46478-seq_length ,  seq_length):\n",
    "             \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data=minibatch_data.to(device)\n",
    "        minibatch_label=minibatch_label.to(device)\n",
    "        \n",
    "        # Detach to prevent from backpropagating all the way to the beginning\n",
    "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.detach()\n",
    "        h=h.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        \n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        utils.normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    eval_on_test_set() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Choose one sentence (taken from the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"some analysts expect oil prices to remain relatively\"\n",
    "\n",
    "sentence2 = \"over the next days and weeks they say investors should look for stocks to\"\n",
    "\n",
    "sentence3 = \"prices averaging roughly $ N a barrel higher in the third\"\n",
    "\n",
    "sentence4 = \"i think my line has been very consistent mrs. hills said at a news\"\n",
    "\n",
    "sentence5 = \"this appears particularly true at gm which had strong sales in\"\n",
    "\n",
    "# or make your own sentence.  No capital letter or punctuation allowed. Each word must be in the allowed vocabulary.\n",
    "sentence6= \"he was very\"\n",
    "\n",
    "# SELECT THE SENTENCE HERE\n",
    "mysentence = sentence3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the sentence into a vector, then send to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1786],\n",
      "        [8705],\n",
      "        [3246],\n",
      "        [ 416],\n",
      "        [  27],\n",
      "        [  35],\n",
      "        [2664],\n",
      "        [ 209],\n",
      "        [ 108],\n",
      "        [  32],\n",
      "        [3017]])\n"
     ]
    }
   ],
   "source": [
    "minibatch_data=utils.sentence2vector(mysentence)\n",
    "      \n",
    "minibatch_data=minibatch_data.to(device)\n",
    "\n",
    "print(minibatch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the initial hidden state to zero, then run the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros(1, 1, hidden_size)\n",
    "# h = torch.zeros(1, hidden_size)\n",
    "\n",
    "h=h.to(device)\n",
    "\n",
    "scores , h = net( minibatch_data , h )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the network prediction for the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prices averaging roughly $ N a barrel higher in the third ... \n",
      "\n",
      "88.6%\t quarter\n",
      "1.5%\t period\n",
      "1.1%\t year\n",
      "0.7%\t of\n",
      "0.4%\t consecutive\n",
      "0.4%\t month\n",
      "0.3%\t term\n",
      "0.3%\t <eos>\n",
      "0.3%\t world\n",
      "0.3%\t fiscal\n",
      "0.3%\t and\n",
      "0.3%\t market\n",
      "0.2%\t session\n",
      "0.2%\t area\n",
      "0.2%\t day\n",
      "0.2%\t rate\n",
      "0.1%\t buy-out\n",
      "0.1%\t week\n",
      "0.1%\t sector\n",
      "0.1%\t half\n",
      "0.1%\t third\n",
      "0.1%\t floor\n",
      "0.1%\t time\n",
      "0.1%\t level\n",
      "0.1%\t nine\n",
      "0.1%\t on\n",
      "0.1%\t section\n",
      "0.1%\t game\n",
      "0.1%\t 1980s\n",
      "0.0%\t industry\n"
     ]
    }
   ],
   "source": [
    "print(mysentence, '... \\n')\n",
    "\n",
    "utils.show_next_word(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
